---
title: "Assigment_5_Answers"
author: "Sonal Savaliya"
date: "November 25, 2018"
output: html_document
---

```{r}
require(lattice)
require("tidyverse")
evals=read.csv('evals.csv')
evals

```

### Explore Data

1. Describe the distribution of score. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not?

#### Ans.1

```{r}
hist(evals$score)
```

Yes, the distribution is left skewed. Most of the students rate around 4.5/5.0, Students did positive evaluation.
If I were a student who want to enroll in this university, I would expected the distrisbution left skewed. It would give me information that there are more good teachers. So bascially, it would help me to take decision that I should enroll in this university.

2. Excluding score, select two other variables and describe their relationship using an appropriate visualization (scatterplot or side-by-side boxplots).

#### Ans.2

```{r}
plot(age~rank,data=evals)
```

In this data and based on above graph, we can say that average age of teaching professor is around 50 years, average age of tenure track professor is around 35 years, and Tenured professor's average age is around 53 years. Tenure track professors group is yogest among all group of professors.
In addition, we can say that tenured professor's range is bigger than teaching and tenure track professors.


### Multiple Linear Regression

```{r}
plot ( evals $ bty_avg  ~  evals $ bty_f1lower )
cor ( evals $ bty_avg ,  evals $ bty_f1lower )
## [1] 0.8439112
```

```{r}
plot ( evals [,  13 : 19 ])
```
3. Is bty_avg still a significant predictor of score? Has the addition of gender to the model changed the parameter estimate for bty_avg?

#### Ans.3

```{r}
m_bty_gen  <-  lm ( score  ~  bty_avg ,  data =  evals )
summary ( m_bty_gen )

```


```{r}

m_bty_gen  <-  lm ( score  ~  bty_avg  +  gender ,  data =  evals )
summary ( m_bty_gen )
```

yes, bty_avg is still significant predictor of score because its p value is less than 0.05. 
Yes, it is too. In fact, gender makes beauty average more signisficant as the p value became smaller.
Here, now estimate for gender is called gendermale.


4. What is the equation of the line corresponding to males? (Hint: For males, the parameter estimate is multiplied by 1.) For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score?

#### Ans.4

$$ Score =  3.74734 + 0.07416 * btyavg + 0.17239 * gendermale  $$
Here, gendermale = 1.

$$ Score =  3.74734 + 0.07416 * btyavg + 0.17239  $$

$$  Score =  3.91973 + 0.07416 * btyavg $$
The equation for females is score = 3.75 + 0.074*bty_avg.
If a male and female professor have the same beauty score, male gender tends to have the higher course evaluation by 0.17.


5. Create a new model called m_bty_rank with gender removed and rank added in. How does R appear to handle categorical variables that have more than two levels? Note that the rank variable has three levels: teaching, tenure track, tenured.

#### Ans.5

```{r}
lm ( score  ~  bty_avg  +  rank ,  data =  evals )
m_bty_rank  <-  lm ( score  ~  bty_avg  +  rank ,  data =  evals )
summary ( m_bty_rank )
```

If categorial varibale has more than 2 levels then R appears the n-1 levels. What I mean, here, we have 3 levels of rank variable(teaching, tenure track, tenured). So, R appears only 2 levels. Here, consider n=3. so R appears n-1 = 3-1 = 2 levels.
I also noticed(not sure) R appears levels which has higher values.For example, here number of tenure track and tenured prof. value is higher than number of teaching prof.(This is about which two levels will appear)

6. The interpretation of the coefficients in multiple regression is slightly different from that of simple regression. The estimate for bty_avg reflects how much higher a group of professors is expected to score if they have a beauty rating that is one point higher while holding all other variables constant. In this case, that translates into considering only professors of the same rank with bty_avg scores that are one point apart.
  
#### Ans.6

```{r}
m_bty_rank = lm ( score  ~ rank + bty_avg,  data =  evals )
m_bty_rank
summary(m_bty_rank)
ggplot(data=evals) +geom_point(aes(x=bty_avg, y =score, color = rank))+geom_smooth(method = "lm",aes(x=bty_avg, y=score, color=rank), fill=NA)
```

When 1 point increases in beauty average, then expected to increase the score of the group of professors by 0.06783.

### The search for the best model

7. Which variable would you expect to have the highest p-value in this model? Why? Hint: Think about which variable would you expect to not have any association with the professor score.

#### Ans.7

Language; Students rated based on quality of teaching and appearance but not base on language (which is school where professor received education).
The language should not have a major association with the professor score.


8.Check your suspicions from the previous exercise. Include the model output in your response.

#### Ans.8


```{r}
m_full  <- lm ( score  ~  rank  +  ethnicity  +
gender  +  language  +  age  +  cls_perc_eval + cls_students  +  cls_level  +  cls_profs  + cls_credits  +  bty_avg +  pic_outfit  +  pic_color ,  data =  evals )
summary ( m_full )
```

The p value for Number of professors variable, is 0.77806 and it is the highest in the model.
The p value for the variable I was expecting to be the highest is 0.03965 for language; which is a big difference.


9. Interpret the coefficient associated with the ethnicity variable.

#### Ans.9

Professors who are not from minority ethnicity tend to score 0.1234929 higher given all other parameters are same. But, given large p-value it's not statistically significant.


10 Drop the variable with the highest p-value and refit the model. Did the coefficients and significance of the other explanatory variables change? (One of the things that makes multiple regression interesting is that coefficient estimates depend on the other variables that are included in the model.) If not, what does this say about whether or not the dropped variable was collinear with the other explanatory variables?

#### Ans.10

```{r}
new_model  <- lm ( score  ~  rank  +  ethnicity  +
gender  +  language  +  age  +  cls_perc_eval + cls_students  +  cls_level  +  cls_credits  +  bty_avg +  pic_outfit  +  pic_color, data =  evals)
summary ( new_model )
```

Yes, all the other explanatory variables's coefficients and significance changed. This indicates that the dropped variable is not collinear with other variables.

11. Using backward-selection and p-value as the selection criterion, determine the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on.

#### Ans.11

```{r}
backward  <- lm ( score  ~  rank  +  ethnicity  +
gender  +  language  +  age  +  cls_perc_eval + cls_students  +  cls_level  +  cls_profs  + cls_credits  +  bty_avg +  pic_outfit  +  pic_color ,  data =  evals )
step(backward, direction = "backward")


final_model = lm(formula = score ~ ethnicity + gender + language + age + cls_perc_eval + 
    cls_credits + bty_avg + pic_outfit + pic_color, data = evals)


summary(final_model)

```

$$ score =  3.907030 + (0.163818 * ethnicity) + (0.202597 * gender) + (-0.246683 * language) + (-0.006925 * age) + (0.004942 * cls_perc_eval) + (0.517205 * cls_credits) + (0.046732 * bty_avg) + (-0.113939 * pic_outfit) +(-0.180870 * pic_color) $$


12. Verify that the conditions for this model are reasonable using diagnostic plots.

#### Ans.12

4 plots: Resiual vs Fitted, Normal Q-Q (Normal Probability Plot), Scale-Location(Spread-Location), Residual vs Leverage

```{r}
plot(final_model)
```


```{r}
hist(final_model$residuals)
```

There's more skew than is desireable, but this isn't a huge concern.

```{r}
plot(residuals(final_model))
```
```{r}
plot(residuals(final_model) ~ evals$ethnicity)
```
Not quite constant variability, close enough. B.c. it's an indicator, its got no choice but to be linear.

```{r}
plot(residuals(final_model) ~ evals$gender)

```

```{r}
plot(residuals(final_model) ~ evals$language)

```

```{r}
plot(residuals(final_model) ~ jitter(evals$age))
```

So, The residuals of the model are nearly normal. The variability of the residuals is nearly constant. The residuals are independent, looks good. each variable is linearly related to the outcome. 